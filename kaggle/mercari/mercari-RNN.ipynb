{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "db0a8d8c-7e97-4d52-a6bd-166cdbcbd1d9",
    "_uuid": "bbcef144213412551d9bec0430ebe0e7802bcbbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenjennhaur/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import time\n",
    "from datetime import datetime \n",
    "start_real = datetime.now()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\n",
    "# from keras.layers import Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "# from nltk.corpus import stopwords\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "# set seed\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "17db3474-7213-4669-814b-e9f158a25887",
    "_uuid": "dec2d2048b62473d869e856613f939bec2906049",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to change to Log ? \n",
    "\n",
    "def rmsle(Y, Y_pred):\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    return np.sqrt(np.mean(np.square(Y_pred - Y )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "0e0be440-f562-4b33-aafc-71929beb780e",
    "_uuid": "21f853dd205340121eddb123f5d713aaa0f311e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.tsv',sep='\\t')\n",
    "test_df = pd.read_csv('test.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "6b251d4c-e5d0-46c0-88d2-97d2fe8c2cf7",
    "_uuid": "750c5401f26ae43793fafce50c0d07198f3569b2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c4509c5c-8837-42e5-84cb-c1ef0a2525d8",
    "_uuid": "bc77ff8bcb17d906487541162a61ad5af6239b97",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get name and description lengths\n",
    "def wordCount(text):\n",
    "    try:\n",
    "        if text == 'No description yet':\n",
    "            return 0\n",
    "        else:\n",
    "            text = text.lower()\n",
    "            words = [w for w in text.split(\" \")]\n",
    "            return len(words)\n",
    "    except: \n",
    "        return 0\n",
    "full_df['desc_len'] = train_df['item_description'].apply(lambda x: wordCount(x))\n",
    "full_df['name_len'] = train_df['name'].apply(lambda x: wordCount(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "5ccffb5f-0ae8-4817-9099-09a00d9314c4",
    "_uuid": "740379f3d787a91bcc10029d9faeb71cffe98c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing data...\n",
      "1    Electronics/Computers & Tablets/Components & P...\n",
      "1              Other/Office supplies/Shipping Supplies\n",
      "Name: category_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# split category name into 3 parts\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "    \n",
    "full_df['subcat_0'], full_df['subcat_1'], full_df['subcat_2'] = \\\n",
    "zip(*full_df['category_name'].apply(lambda x: split_cat(x)))\n",
    "\n",
    "# Filling missing values\n",
    "def fill_missing_values(df):\n",
    "    df.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    df.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    df.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    df.item_description.replace('No description yet',\"missing\", inplace=True)\n",
    "    return df\n",
    "\n",
    "print(\"Filling missing data...\")\n",
    "full_df = fill_missing_values(full_df)\n",
    "print(full_df.category_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "8b8746ed-1b65-40be-96de-e87975ce8831",
    "_uuid": "4f9fd2f8df47338eb07241bd4f3f5ae1a8137643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928207\n"
     ]
    }
   ],
   "source": [
    "all_brands = set(full_df['brand_name'].values)\n",
    "\n",
    "# Get missing brand name from name\n",
    "premissing = len(full_df.loc[full_df['brand_name'] == 'missing'])\n",
    "def brandfinder(line):\n",
    "    brand = line[0]\n",
    "    name = line[1]\n",
    "    namesplit = name.split(' ')\n",
    "    if brand == 'missing':\n",
    "        for x in namesplit:\n",
    "            if x in all_brands:\n",
    "                return name\n",
    "    if name in all_brands:\n",
    "        return name\n",
    "    return brand\n",
    "full_df['brand_name'] = train_df[['brand_name','name']].apply(brandfinder, axis = 1)\n",
    "found = premissing-len(full_df.loc[train_df['brand_name'] == 'missing'])\n",
    "print(found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "8886d95a-3d4e-4d57-bd53-bf814b76f49a",
    "_uuid": "fde1716492ced542a18e6c8488d257dc2c8c6ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of brands 4823\n",
      "number of item condition 5\n",
      "number of cat1 11\n",
      "number of cat2 114\n",
      "number of cat3 883\n"
     ]
    }
   ],
   "source": [
    "print('number of brands', len(full_df.brand_name.unique()))\n",
    "print('number of item condition', len(full_df.item_condition_id.unique()))\n",
    "print('number of cat1', len(full_df.subcat_0.unique()))\n",
    "print('number of cat2', len(full_df.subcat_1.unique()))\n",
    "print('number of cat3', len(full_df.subcat_2.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "01d6b834-3887-4fc6-99be-f7e240f2f650",
    "_uuid": "77ebeee9d25278b39065cdf1c81c5da11583d125",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "full_df['shipping'] = full_df['shipping'].astype('int')\n",
    "full_df[\"target\"] = np.log1p(full_df.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "27d213d5-3eee-474a-a8ca-afb2dfb01a0c",
    "_uuid": "e33959a286e0be034312638892e0729ae0218725"
   },
   "source": [
    "**RNN******\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "16585fde-f2a6-44e2-b90a-cf96ebc69e80",
    "_uuid": "b7b03d36012aa1e60544c6e8c0a5bc4b4c4a765f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming text data to sequences...\n",
      "   Fitting tokenizer...\n",
      "   Transforming text to sequences...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming text data to sequences...\")\n",
    "raw_text = np.hstack([full_df.item_description.str.lower(), full_df.name.str.lower(), full_df.category_name.str.lower()])\n",
    "\n",
    "print(\"   Fitting tokenizer...\")\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "print(\"   Transforming text to sequences...\")\n",
    "full_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\n",
    "full_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n",
    "# full_df['seq_category'] = tok_raw.texts_to_sequences(full_df.category_name.str.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "cd8448ae-c485-4201-9a06-cdf2b7c5e49b",
    "_uuid": "e32cd88d53901602e55626d9376bdaf72db6bd65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['brand_name', 'category_name', 'item_condition_id', 'item_description',\n",
       "       'name', 'price', 'shipping', 'test_id', 'train_id', 'desc_len',\n",
       "       'name_len', 'subcat_0', 'subcat_1', 'subcat_2', 'target',\n",
       "       'seq_item_description', 'seq_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "2e2c48ff-b994-4893-9cb1-9dc3b4d2b8ba",
    "_uuid": "9969e67a5c9c5f96c6babd739270ea504d1ddb9b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Steps\n",
    "# Update rnn_mapper\n",
    "# update get_rnn_data\n",
    "# Add input to model and also the final model = Model(input,outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b0184ec7-560a-411f-9e93-98e183bc9c1c",
    "_uuid": "b9379b66ef88ad2ad9dc5b38440dddbb6673bed9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_mapper = DataFrameMapper([\n",
    "    ('seq_name',None),\n",
    "    ('seq_item_description',None),\n",
    "    ('brand_name',LabelEncoder()),\n",
    "    ('item_condition_id',None),\n",
    "    ('desc_len',None),\n",
    "    ('name_len',None),\n",
    "    ('shipping',None),\n",
    "    ('subcat_0',LabelEncoder()),\n",
    "    ('subcat_1',LabelEncoder()),\n",
    "    ('subcat_2',LabelEncoder()),\n",
    "    ('train_id',None),\n",
    "    ('test_id',None),\n",
    "    ('target',None)\n",
    "],df_out=True)\n",
    "\n",
    "rnn_df = rnn_mapper.fit_transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "e0050e3d-9603-4d30-b3e4-4e7dc43084d4",
    "_uuid": "2d3260477b10494baa6739460591f084da1ff705",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = rnn_df[pd.isnull(rnn_df.test_id)].copy()\n",
    "del trainset['test_id']\n",
    "del trainset['train_id']\n",
    "testset = rnn_df[pd.isnull(rnn_df.train_id)].copy()\n",
    "del testset['train_id']\n",
    "del testset['test_id']\n",
    "del testset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "c5f49b81-db4a-4351-845d-0843998fc6ab",
    "_uuid": "f07e07336ba62ed440703487f7a36bd79e1a4709",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NAME_SEQ = 17 #17\n",
    "MAX_ITEM_DESC_SEQ = 269 #269\n",
    "MAX_TEXT = np.max([\n",
    "    np.max(rnn_df.seq_name.max()),\n",
    "    np.max(rnn_df.seq_item_description.max()),\n",
    "#     np.max(full_df.seq_category.max()),\n",
    "]) + 100\n",
    "MAX_BRAND = np.max(rnn_df.brand_name.max()) + 1\n",
    "MAX_CONDITION = np.max(rnn_df.item_condition_id.max()) + 1\n",
    "# MAX_DESC_LEN = np.max(rnn_df.desc_len.max()) + 1\n",
    "# MAX_NAME_LEN = np.max(rnn_df.name_len.max()) + 1\n",
    "MAX_SUBCAT_0 = np.max(rnn_df.subcat_0.max()) + 1\n",
    "MAX_SUBCAT_1 = np.max(rnn_df.subcat_1.max()) + 1\n",
    "MAX_SUBCAT_2 = np.max(rnn_df.subcat_2.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "890d9336-f77b-40f9-a6cc-c53d12fe71e2",
    "_uuid": "99fe1c9ee6a4edb406e9f929f8842044b8a1d8b9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ),\n",
    "        'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n",
    "        'brand_name': np.array(dataset.brand_name),\n",
    "#         'category': np.array(dataset.category),\n",
    "#         'category_name': pad_sequences(dataset.seq_category, maxlen=MAX_CATEGORY_SEQ),\n",
    "        'item_condition': np.array(dataset.item_condition_id),\n",
    "        'num_vars': np.array(dataset.shipping),\n",
    "        'desc_len': np.array(dataset.desc_len),\n",
    "        'name_len': np.array(dataset.name_len),\n",
    "        'subcat_0': np.array(dataset.subcat_0),\n",
    "        'subcat_1': np.array(dataset.subcat_1),\n",
    "        'subcat_2': np.array(dataset.subcat_2),\n",
    "    }\n",
    "    return X\n",
    "\n",
    "\n",
    "train,valid = train_test_split(trainset, random_state=123, train_size=0.8)\n",
    "X_test = testset.values\n",
    "\n",
    "X_train = get_rnn_data(train.iloc[:,0:-1].copy())\n",
    "y_train = train.iloc[:,-1].values\n",
    "# Y_train = train.target.values.reshape(-1, 1)\n",
    "\n",
    "X_valid = get_rnn_data(valid.iloc[:,0:-1].copy())\n",
    "y_valid = valid.iloc[:,-1].values\n",
    "# Y_dev = dev.target.values.reshape(-1, 1)\n",
    "\n",
    "X_test = get_rnn_data(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEWLY ADDED\n",
    "\n",
    "def rmsle_K(y, y0):\n",
    "    return K.sqrt(K.mean(K.square(tf.log1p(y) - tf.log1p(y0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "91430828-afb8-42e3-8018-8d1fbce8cd2a",
    "_uuid": "a9fbe39271fbafb2490e9064a5158b20e94d012f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN\n",
    "\n",
    "# set seed again in case testing models adjustments by looping next 2 blocks\n",
    "np.random.seed(123)\n",
    "\n",
    "def new_rnn_model(lr=0.001, decay=0.0,dropout=0.3):\n",
    "    # Inputs\n",
    "    name = Input(shape=(17,), name=\"name\")\n",
    "    item_desc = Input(shape=(269,), name=\"item_desc\")\n",
    "    brand_name = Input(shape=(1,), name=\"brand_name\")\n",
    "    item_condition = Input(shape=(1,), name=\"item_condition\")\n",
    "    num_vars = Input(shape=(1,), name=\"num_vars\")\n",
    "    desc_len = Input(shape=[1], name=\"desc_len\")\n",
    "    name_len = Input(shape=[1], name=\"name_len\")\n",
    "    subcat_0 = Input(shape=(1,), name=\"subcat_0\")\n",
    "    subcat_1 = Input(shape=(1,), name=\"subcat_1\")\n",
    "    subcat_2 = Input(shape=(1,), name=\"subcat_2\")\n",
    "\n",
    "    # Embeddings layers (adjust outputs to help model)\n",
    "    emb_name = Embedding(MAX_TEXT, 20)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "#     emb_category_name = Embedding(MAX_TEXT, 20)(category_name)\n",
    "#     emb_category = Embedding(MAX_CATEGORY, 10)(category)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "#     emb_desc_len = Embedding(MAX_DESC_LEN, 5)(desc_len)\n",
    "#     emb_name_len = Embedding(MAX_NAME_LEN, 5)(name_len)\n",
    "    emb_subcat_0 = Embedding(MAX_SUBCAT_0, 10)(subcat_0)\n",
    "    emb_subcat_1 = Embedding(MAX_SUBCAT_1, 10)(subcat_1)\n",
    "    emb_subcat_2 = Embedding(MAX_SUBCAT_2, 10)(subcat_2)\n",
    "    \n",
    "\n",
    "    # rnn layers (GRUs are faster than LSTMs and speed is important here)\n",
    "    rnn_layer1 = GRU(16) (emb_item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_name)\n",
    "#     rnn_layer3 = GRU(8) (emb_category_name)\n",
    "\n",
    "    # CNN Layer\n",
    "#     cnn_layer1 = Conv1D(filters=16, kernel_size=3, activation='relu') (emb_item_desc)\n",
    "#     cnn_layer1 = GlobalMaxPooling1D()(cnn_layer1)\n",
    "\n",
    "#     cnn_layer2 = Conv1D(filters=8, kernel_size=3, activation='relu')(emb_name)\n",
    "#     cnn_layer2 = GlobalMaxPooling1D()(cnn_layer2)\n",
    "\n",
    "# Another option of CNN Layer\n",
    "# x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(35)(x)  # global max pooling\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    # main layers\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand_name)\n",
    "#         , Flatten() (emb_category)\n",
    "        , Flatten() (emb_item_condition)\n",
    "#         , Flatten() (emb_desc_len)\n",
    "#         , Flatten() (emb_name_len)\n",
    "        , Flatten() (emb_subcat_0)\n",
    "        , Flatten() (emb_subcat_1)\n",
    "        , Flatten() (emb_subcat_2)\n",
    "        , rnn_layer1\n",
    "        , rnn_layer2\n",
    "#         , rnn_layer3\n",
    "        , num_vars\n",
    "    ])\n",
    "    # (incressing the nodes or adding layers does not effect the time quite as much as the rnn layers)\n",
    "    main_l = Dropout(dropout)(Dense(512,kernel_initializer='normal',activation='relu') (main_l))\n",
    "    main_l = Dropout(dropout)(Dense(256,kernel_initializer='normal',activation='relu') (main_l))\n",
    "    main_l = Dropout(dropout)(Dense(128,kernel_initializer='normal',activation='relu') (main_l))\n",
    "    main_l = Dropout(dropout)(Dense(64,kernel_initializer='normal',activation='relu') (main_l))\n",
    "\n",
    "    # the output layer.\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    model = Model([name, item_desc, brand_name , item_condition,num_vars,desc_len,name_len\n",
    "                   , subcat_0, subcat_1, subcat_2], output)\n",
    "\n",
    "    optimizer = Adam(lr=lr, decay=decay)\n",
    "    # (mean squared error loss function works as well as custom functions)  \n",
    "    model.compile(loss = 'mse' , optimizer = optimizer, metrics=[rmsle_K])\n",
    "\n",
    "    return model\n",
    "\n",
    "# model = new_rnn_model()\n",
    "# model.summary()\n",
    "# del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track = pd.DataFrame(columns=['p_lr','p_epoch','p_batch','p_drop','loss','val_loss','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_lr = [0.005]\n",
    "p_epochs = [3]\n",
    "p_batch = [512,2048]\n",
    "p_drop = [0.1,0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "params = product(p_lr,p_epoch,p_batch,p_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(params))\n",
    "# params = [(0.005,3,512,0.1),\n",
    "#           (0.01,3,512,0.1),\n",
    "#           (0.007,3,512,0.1)]\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e1fb769e-9454-4106-b832-37eca308d640",
    "_uuid": "324178f83d1cbe972c28c2998fd800f6943d61ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.005, 3, 512, 0.1)\n",
      "Train on 1186028 samples, validate on 296507 samples\n",
      "Epoch 1/3\n",
      " 498688/1186028 [===========>..................] - ETA: 4:12 - loss: 0.4614 - rmsle_K: 0.1658"
     ]
    }
   ],
   "source": [
    "# Set hyper parameters for the model.\n",
    "# p_lr = 0.005\n",
    "# p_batch = 512 * 3\n",
    "# p_epochs = 3\n",
    "# p_drop = 0.3\n",
    "\n",
    "for i in params:\n",
    "    print(i)\n",
    "    p_lr, p_epochs,p_batch,p_drop = i\n",
    "\n",
    "    exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "    steps = int(len(train) / p_batch) * p_epochs\n",
    "    lr_init, lr_fin = p_lr, 0.001\n",
    "    lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "\n",
    "    # Create model and fit it with training dataset.\n",
    "    # rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n",
    "    rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    history =  rnn_model.fit(\n",
    "            X_train, y_train, epochs=p_epochs, batch_size=p_batch,\n",
    "            validation_data=(X_valid, y_valid), verbose=1,\n",
    "    )\n",
    "\n",
    "    run_time = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % run_time)\n",
    "    # Track\n",
    "    track.loc[len(track)]=[p_lr,p_epochs,p_batch,p_drop,history.history['loss'][-1],history.history['val_loss'][-1],run_time] \n",
    "\n",
    "\n",
    "    print(history.history.keys())\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track.to_csv('results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_lr</th>\n",
       "      <th>p_epoch</th>\n",
       "      <th>p_batch</th>\n",
       "      <th>p_drop</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.166646</td>\n",
       "      <td>0.196117</td>\n",
       "      <td>1364.526331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.170712</td>\n",
       "      <td>0.193605</td>\n",
       "      <td>1355.465158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.192080</td>\n",
       "      <td>1355.928638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    p_lr  p_epoch  p_batch  p_drop      loss  val_loss         time\n",
       "0  0.005      3.0    512.0     0.1  0.166646  0.196117  1364.526331\n",
       "1  0.010      3.0    512.0     0.1  0.170712  0.193605  1355.465158\n",
       "2  0.007      3.0    512.0     0.1  0.168500  0.192080  1355.928638"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try using msle vs mse as metrics\n",
    "\n",
    "# Dropout = 0.3 , Epoch = 3 , Init = 0.005 => 0.001 , BATCH SIZE = 1536\n",
    "# Loss = 0.22 ,  Val_loss = 0.1966\n",
    "\n",
    "# Dropout = 0.3 , Epoch = 5 , Init = 0.005 => 0.001 , BATCH SIZE = 1536\n",
    "# Loss = 0.1607 ,  Val_loss = 0.1964\n",
    "\n",
    "# Dropout = 0.5 , Epoch = 3 , Init = 0.005 => 0.001 , BATCH SIZE = 1536\n",
    "# Loss = 0.2497 ,  Val_loss = 0.1977\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cb8bc9ea-1570-451e-8179-df5268e7414b",
    "_uuid": "66ab15d60aa58f67c6c2e5cf5fb0acf795ed2687",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = rnn_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0ef79b7a-897a-47a9-aba9-1e6c1b662dee",
    "_uuid": "a67b1ba2b470792db820154fdc6fbd392614d57a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds)\n",
    "preds_df.reset_index(inplace=True)\n",
    "preds_df.columns = ['test_id','price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6c54af1d-69db-48e8-b75e-dc44dd9976e8",
    "_uuid": "b70a76e364c7425eae3ebb96122f88d8b690b44c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_df['price'] = preds_df.price.apply(lambda x:np.expm1(x))\n",
    "preds_df.to_csv('rnn_submit_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ded46e0-ee38-450a-b854-79f98ade8c58",
    "_uuid": "465d07aac1ff893cd54b1b4ab47ef71db25687d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/anttttti/Wordbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperas.distributions import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
