{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Role Title and Role Code are similar based on pandas\n",
    "*Instead of using hashing, can consider cantor(m,n) = 1/2 * (m+n)(m+n+1)+m. Beware of hash collision\n",
    "*PCA / SVD are more suited for continuous values. For discrete variables, consider using MCA\n",
    "1. Feature Extraction - Using Genetic Algorithm / Greedy Algorithm [sklearn.feature_selection.RFE, RFECV , Gradient Boosting\n",
    "2. Try using Stacked Classifier \n",
    "  * np.mean\n",
    "  * weighted mean\n",
    "  * jaccard score\n",
    "  * pass the results from each classifer into another RF / Linear Regression\n",
    "3. Data Exploratory - Find if train has more classes compared to test ? \n",
    "4. Try to run on Spark\n",
    "5. Stratified K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pivottablejs import pivot_ui\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt  \n",
    "from ipywidgets import widgets\n",
    "import numpy as np\n",
    "from sklearn import (metrics, cross_validation, linear_model, preprocessing)\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import (GradientBoostingClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import ensemble\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sb.set(style=\"ticks\", color_codes=True)\n",
    "df = pd.read_csv('train.csv')\n",
    "dft = pd.read_csv('test.csv')\n",
    "#g = sb.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Observation Only\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(df['RESOURCE'].value_counts())\n",
    "#df['RESOURCE'].plot.hist(bins=7518)\n",
    "\n",
    "#dfc = df.iloc[:,1:10] \n",
    "dfc = df.iloc[:,:]\n",
    "for col in df.columns:\n",
    "  dfc[col] = df[col].astype('category')\n",
    "\n",
    "dfc.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Observation Only\n",
    "#test = pd.pivot_table(dfc,index=['ROLE_TITLE','ROLE_CODE'],aggfunc='count',dropna=True)\n",
    "#dfc.pivot_table(index='ROLE_TITLE',aggfunc=lambda x:len(x.unique()))\n",
    "mult = {}\n",
    "for c in df.columns:\n",
    "    temp = df.pivot_table(index=c,aggfunc=lambda x:len(x.unique()),fill_value=0).apply(np.max)\n",
    "    mult[c] = temp\n",
    "\n",
    "multpd = pd.DataFrame(mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_results(predictions, filename):\n",
    "    \"\"\"Given a vector of predictions, save results in CSV format.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"id,ACTION\\n\")\n",
    "        for i, pred in enumerate(predictions):\n",
    "            f.write(\"%d,%f\\n\" % (i + 1, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cantor(pairs):\n",
    "    result2 = 0.5 * (pairs[0] + pairs[1]) * (pairs[0] + pairs[1] +1) + pairs[1]\n",
    "    if(len(pairs) == 2): \n",
    "        return result2\n",
    "    if(len(pairs) == 3 ):\n",
    "        result3 = 0.5 * (result2 + pairs[2]) * (result2 + pairs[2] +1) + pairs[2]\n",
    "        return result3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert from dataframe to np.array\n",
    "y = np.ravel(df.iloc[:,:1])\n",
    "X = np.array(df.iloc[:,1:9])\n",
    "X_test = np.array(dft.iloc[:,1:9])\n",
    "header = list(df.columns[1:])\n",
    "\n",
    "data = np.vstack((X,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# yc = combinations(range(n),2)\n",
    "# to view all the combinations\n",
    "# list(yc) \n",
    "\n",
    "#train data[:32769,:]\n",
    "#test data[32769:,:]\n",
    "#tuple is used here so that the result is hashable\n",
    "\n",
    "test_offset = 32769\n",
    "# Get Combinations of 2 features \n",
    "for indices in combinations(range(8),2):\n",
    "    #data = np.column_stack((data,list(ctypes.c_size_t(hash(tuple(v))).value for v in data[:,indices]))) \n",
    "    #data = np.column_stack((data,list(np.uint64(hash(tuple(v))) for v in data[:,indices]))) \n",
    "    #data = np.column_stack((data,list(hash(tuple(v)) for v in data[:,indices]))) \n",
    "    data = np.column_stack((data,list(cantor(tuple(v)) for v in data[:,indices]))) \n",
    "    #a,b = indices\n",
    "    #header.append(header[a] +\"&\"+ header[b])\n",
    "print('2 Combinations')\n",
    "print(data.shape)\n",
    "print(np.any(data < 0))\n",
    "print(np.ndarray.min(data))\n",
    "print(np.ndarray.max(data))\n",
    "    \n",
    "# Get Combinations of 3 features \n",
    "for indices in combinations(range(8),3):\n",
    "    data = np.column_stack((data,list(cantor(tuple(v)) for v in data[:,indices]))) \n",
    "    \n",
    "#Check loaded correctly with non-zero elements    \n",
    "print('3 Combinations')\n",
    "print(data.shape)\n",
    "print(np.any(data < 0))\n",
    "print(np.ndarray.min(data))\n",
    "print(np.ndarray.max(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Simplify the encoding. Probably not essential. Could possibly improve performance\n",
    "# Vertical Stacking is required so that all the features are captured. There might be some features in test set not in train set\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "data_e = []\n",
    "# To perform column wise operations, use Transpose function\n",
    "for i in data.T:\n",
    "    le.fit(i)\n",
    "    data_e.append(le.transform(i))\n",
    "\n",
    "data = np.asarray(data_e).T\n",
    "\n",
    "print(np.any(data < 0))\n",
    "print(np.ndarray.min(data))\n",
    "print(np.ndarray.max(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Code to help Visualize what the algorithm is doing \n",
    "# t_data = []\n",
    "# test_data = np.array([[1,2,3,4],[1,2,2,0],[1,2,3,4],[1,4,5,6],[2,3,4,5]])\n",
    "# print(test_data.shape)\n",
    "\n",
    "# #if the combination of categories are the same, they will be hashed to the same value. Eg 1 row with (0,1,0) will have same \n",
    "# #hash value of another row with (0,1,0)\n",
    "# for indices in combinations(range(4),2):\n",
    "#     t_data.append(hash(tuple(v)) for v in test_data[:,indices])\n",
    "#     #print(v for v in test_data[:,indices])\n",
    "#     print(indices)\n",
    "    \n",
    "# #Comparison 1 - Combinations are (0,1),(0,2),(0,3),(1,2),(1,3),(2,3)\n",
    "# # for v in test_data[:,(0,1)]:\n",
    "# #   print(hash(tuple(v)))\n",
    "\n",
    "# #Comparison 2 - t_data[0] corresponds to (0,1)\n",
    "# #tuple(t_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.OneHotEncoder(dtype=np.uint32)\n",
    "# we want to encode the category IDs encountered both in\n",
    "# the training and the test set, so we fit the encoder on both\n",
    "encoder.fit(data)\n",
    "X = encoder.transform(data[:test_offset,:])\n",
    "X_test = encoder.transform(data[test_offset:,:])\n",
    "\n",
    "# encoder = preprocessing.OneHotEncoder()\n",
    "# encoder.fit(data)\n",
    "# X = encoder.transform(X)\n",
    "# X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Total One Hot Features\",encoder.active_features_)\n",
    "print(\"Boundaries of each Feature\",len(encoder.feature_indices_),encoder.feature_indices_)\n",
    "print(\"Number of unique values in each feature\",len(encoder.n_values_),encoder.n_values_)\n",
    "#np.unique(data[:,0])\n",
    "sb.barplot(x=np.arange(92),y=encoder.n_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'encoder' (OneHotEncoder)\n",
      "Stored 'data' (ndarray)\n",
      "Stored 'X' (csr_matrix)\n",
      "Stored 'X_test' (csr_matrix)\n",
      "Stored 'y' (ndarray)\n",
      "Stored 'features' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "#inside a ipython/nb session\n",
    "%store encoder\n",
    "%store data\n",
    "%store X\n",
    "%store X_test\n",
    "%store y\n",
    "%store features\n",
    "\n",
    "# %store -r encoder\n",
    "# %store -r data\n",
    "# %store -r X\n",
    "# %store -r X_test\n",
    "# %store -r y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.         ...,  0.          0.          0.00328731]\n"
     ]
    }
   ],
   "source": [
    "#Feature Selection - Gradient Boosting (Filtering)\n",
    "dtc = DecisionTreeClassifier(criterion='entropy')\n",
    "dtc.fit(X,y)\n",
    "print(dtc.feature_importances_)\n",
    "features = dtc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sb.distplot(feat,bins=100)\n",
    "#print(np.argmax(features),np.max(features))\n",
    "#nfeatures=np.asarray(features)\n",
    "np.where(features > 0.002)[0].size\n",
    "#feat = features * 1000\n",
    "# Sum features in the same bin as defined by encoder.n_values_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature Selection - Greedy Algorithm (Wrapping)\n",
    "\n",
    "estimator = linear_model.LogisticRegression(C=1)\n",
    "selector = RFE(estimator)\n",
    "selector = selector.fit(X, y)\n",
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "mean_auc = 0.0\n",
    "\n",
    "n = 10  # repeat the CV procedure 10 times to get more precise results\n",
    "model = linear_model.LogisticRegression(C=1)\n",
    "#model = BernoulliNB(alpha=0.005)\n",
    "for i in range(n):\n",
    "    # for each iteration, randomly hold out 20% of the data as CV set\n",
    "    X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(X, y, test_size=.20, random_state=i*SEED)\n",
    "\n",
    "    # if you want to perform feature selection / hyperparameter\n",
    "    # optimization, this is where you want to do it\n",
    "\n",
    "    # train model and make predictions\n",
    "    model.fit(X_train, y_train) \n",
    "    preds = model.predict_proba(X_cv)[:, 1]\n",
    "\n",
    "    # compute AUC metric for this CV fold\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_cv, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(\"AUC (fold %d/%d): %f\" % (i + 1, n, roc_auc))\n",
    "    mean_auc += roc_auc\n",
    "\n",
    "print(\"Mean AUC: %f\" % (mean_auc/n))\n",
    "\n",
    "# === Predictions === #\n",
    "# When making predictions, retrain the model on the whole training set\n",
    "model.fit(X, y)\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "save_results(preds, \"submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy Operations\n",
    "# print(np.fromiter(t_data[0],np.int32))\n",
    "# print(np.fromiter(t_data[1],np.int32))\n",
    "# print(np.fromiter(t_data[2],np.int32))\n",
    "# print(np.fromiter(t_data[3],np.int32))\n",
    "# print(np.fromiter(t_data[4],np.int32))\n",
    "# print(np.fromiter(t_data[5],np.int32))\n",
    "\n",
    "# Convert to numpy array - Use np.array(list)\n",
    "# for i in nparray - iterate through rows\n",
    "# for i in nparray.T - iterate through columns\n",
    "# Initialize np.zeros(5)\n",
    "\n",
    "# To view generator operators (Use list,tuples)\n",
    "# for i,j in enumerate(t_data):\n",
    "#     #lst_array[i] = np.array(list(j))\n",
    "#     lst_array[i] = np.fromiter(j,np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC = Logistic Regression returns the probability that a certain input features would be in a certain class\n",
    "ROC curve visualizaes all possible thresholds\n",
    "Misclassification rate is error rate for a single threshold\n",
    "\n",
    "\n",
    "Attempt with original features : Public : 0.88515, Private : 0.88205\n",
    "Attempt with original features + combination of 2 : Public : 0.90141, Private 0.89582 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
