{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# Improved LSTM baseline\n",
    "\n",
    "This kernel is a somewhat improved version of [Keras - Bidirectional LSTM baseline](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051) along with some additional documentation of the steps. (NB: this notebook has been re-run on the new test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvBlockLayer(object):\n",
    "    \"\"\"\n",
    "    two layer ConvNet. Apply batch_norm and relu after each layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, num_filters):\n",
    "        self.model = Sequential()\n",
    "        # first conv layer\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\", input_shape=input_shape))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        # second conv layer\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "EMBEDDING_FILE=f'wv/glove.6B.100d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 20 # char embed\n",
    "maxlen = 1024 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "# test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "# list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# cachedStop =  stopwords.words('english')\n",
    "# pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "# def cleanwords(sent):\n",
    "#     return ' '.join([word.lower() for word in sent.lower().split() if word not in cachedStop ])\n",
    "    # return pattern.sub('', sent.lower())\n",
    "\n",
    "# def cleanchars(sent):\n",
    "#     return sent.translate(translator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
    "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
   },
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "# list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "# X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(tokenizer.word_counts)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2336"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))+1\n",
    "# embedding_matrix = np.random.normal(0, 1, (nb_words, embed_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: continue # greater than max word features\n",
    "#     embedding_vector = embeddings_index.get(word) # out of word vocabulary\n",
    "#     if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(num_filters, num_classes, sequence_max_length=1024, num_quantized_chars=71, embedding_size=16, learning_rate=0.001, top_k=3, model_path=None):\n",
    "\n",
    "    inputs = Input(shape=(sequence_max_length, ), dtype='int32', name='inputs')\n",
    "\n",
    "    embedded_sent = Embedding(num_quantized_chars, embedding_size, input_length=sequence_max_length)(inputs)\n",
    "\n",
    "    # First conv layer\n",
    "    conv = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(embedded_sent)\n",
    "\n",
    "    # Each ConvBlock with one MaxPooling Layer\n",
    "#     for i in range(len(num_filters)):\n",
    "#         conv = ConvBlockLayer(get_conv_shape(conv), num_filters[i])(conv)\n",
    "#         conv = MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(conv)\n",
    "\n",
    "    for i in range(len(num_filters)):\n",
    "        conv = Conv1D(filters=num_filters[i], kernel_size=3, strides=1, padding=\"same\")(conv)\n",
    "        conv = BatchNormalization()(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "\n",
    "        conv = Conv1D(filters=num_filters[i], kernel_size=3, strides=1, padding=\"same\")(conv)\n",
    "        conv = BatchNormalization()(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "\n",
    "        conv = MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(conv)\n",
    "    \n",
    "    conv = GlobalAveragePooling1D()(conv)\n",
    "    # k-max pooling (Finds values and indices of the k largest entries for the last dimension)\n",
    "#     def _top_k(x):\n",
    "#         x = tf.transpose(x, [0, 2, 1])\n",
    "#         k_max = tf.nn.top_k(x, k=top_k)\n",
    "#         return tf.reshape(k_max[0], (-1, num_filters[-1] * top_k))\n",
    "#     k_max = Lambda(_top_k, output_shape=(num_filters[-1] * top_k,))(conv)\n",
    "\n",
    "    # 3 fully-connected layer with dropout regularization\n",
    "    fc1 = Dropout(0.2)(Dense(512, activation='relu', kernel_initializer='he_normal')(conv))\n",
    "    fc2 = Dropout(0.2)(Dense(512, activation='relu', kernel_initializer='he_normal')(fc1))\n",
    "    fc3 = Dense(num_classes, activation='sigmoid')(fc2)\n",
    "\n",
    "    # define optimizer\n",
    "#     sgd = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=False)\n",
    "    model = Model(inputs=inputs, outputs=fc3)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    if model_path is not None:\n",
    "        model.load_weights(model_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = [64, 128, 256, 512]\n",
    "model = build_model(num_filters=num_filters, num_classes=6, embedding_size=embed_size, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 1024, 20)          1420      \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 512, 64)           3904      \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 512, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 512, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 512, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 512, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 512, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 512, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 256, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 256, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 256, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 256, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 256, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 128, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 128, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 128, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 128, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 64, 512)           393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 64, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 64, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 64, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,116,178\n",
      "Trainable params: 2,112,338\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a624b55-3720-42bc-ad5a-7cefc76d83f6",
    "_uuid": "e2a0e9ce12e1ff5ea102665e79de23df5caf5802"
   },
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when not submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
    "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      " 18400/143613 [==>...........................] - ETA: 1:48 - loss: 0.1249 - acc: 0.9645"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
    "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
   },
   "source": [
    "And finally, get predictions for the test set and prepare a submission CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "28ce30e3-0f21-48e5-af3c-7e5512c9fbdc",
    "_uuid": "e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 49s 321us/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('glove300.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample_submission.to_csv('base_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_submission = pd.read_csv('data/sample_submission.csv')\n",
    "# len(test_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline Score\n",
    "# loss: 0.0417 - acc: 0.9840 - val_loss: 0.0451 - val_acc: 0.9829 --> AUC : 0.9787\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
