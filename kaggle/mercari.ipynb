{
  "cells": [
    {
      "metadata": {
        "_uuid": "bbcef144213412551d9bec0430ebe0e7802bcbbe",
        "_cell_guid": "db0a8d8c-7e97-4d52-a6bd-166cdbcbd1d9",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport wordbatch \n\nfrom datetime import datetime \nstart_real = datetime.now()\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn_pandas import DataFrameMapper, cross_val_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\n# from keras.layers import Bidirectional\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom nltk.corpus import stopwords\nimport math\n# set seed\nnp.random.seed(123)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Try wordbatch\n# Try RNN\n# Look at RMSLE Error Function\n# https://www.kaggle.com/valkling/mercari-rnn-2ridge-models-with-notes-0-42755\n# Combine both sets and do processing together. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "dec2d2048b62473d869e856613f939bec2906049",
        "_cell_guid": "17db3474-7213-4669-814b-e9f158a25887",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Need to change to Log ? \n\ndef rmsle(Y, Y_pred):\n    assert Y.shape == Y_pred.shape\n    return np.sqrt(np.mean(np.square(Y_pred - Y )))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "21f853dd205340121eddb123f5d713aaa0f311e4",
        "_cell_guid": "0e0be440-f562-4b33-aafc-71929beb780e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv('../input/train.tsv',sep='\\t')\ntest_df = pd.read_csv('../input/test.tsv',sep='\\t')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "750c5401f26ae43793fafce50c0d07198f3569b2",
        "_cell_guid": "6b251d4c-e5d0-46c0-88d2-97d2fe8c2cf7",
        "trusted": false
      },
      "cell_type": "code",
      "source": "full_df = pd.concat([train_df,test_df])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "bc77ff8bcb17d906487541162a61ad5af6239b97",
        "_cell_guid": "c4509c5c-8837-42e5-84cb-c1ef0a2525d8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# get name and description lengths\ndef wordCount(text):\n    try:\n        if text == 'No description yet':\n            return 0\n        else:\n            text = text.lower()\n            words = [w for w in text.split(\" \")]\n            return len(words)\n    except: \n        return 0\nfull_df['desc_len'] = train_df['item_description'].apply(lambda x: wordCount(x))\nfull_df['name_len'] = train_df['name'].apply(lambda x: wordCount(x))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "740379f3d787a91bcc10029d9faeb71cffe98c88",
        "_cell_guid": "5ccffb5f-0ae8-4817-9099-09a00d9314c4",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# split category name into 3 parts\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")\n    \nfull_df['subcat_0'], full_df['subcat_1'], full_df['subcat_2'] = \\\nzip(*full_df['category_name'].apply(lambda x: split_cat(x)))\n\n# Filling missing values\ndef fill_missing_values(df):\n    df.category_name.fillna(value=\"missing\", inplace=True)\n    df.brand_name.fillna(value=\"missing\", inplace=True)\n    df.item_description.fillna(value=\"missing\", inplace=True)\n    df.item_description.replace('No description yet',\"missing\", inplace=True)\n    return df\n\nprint(\"Filling missing data...\")\nfull_df = fill_missing_values(full_df)\nprint(full_df.category_name[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4f9fd2f8df47338eb07241bd4f3f5ae1a8137643",
        "_cell_guid": "8b8746ed-1b65-40be-96de-e87975ce8831",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "all_brands = set(full_df['brand_name'].values)\n\n# Get missing brand name from name\npremissing = len(full_df.loc[full_df['brand_name'] == 'missing'])\ndef brandfinder(line):\n    brand = line[0]\n    name = line[1]\n    namesplit = name.split(' ')\n    if brand == 'missing':\n        for x in namesplit:\n            if x in all_brands:\n                return name\n    if name in all_brands:\n        return name\n    return brand\nfull_df['brand_name'] = train_df[['brand_name','name']].apply(brandfinder, axis = 1)\nfound = premissing-len(full_df.loc[train_df['brand_name'] == 'missing'])\nprint(found)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "61907173edd0ae69c9e6f37d0a88e1460e7f2b57",
        "_cell_guid": "ec678654-9902-4fa1-a6d1-216079d267d3",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "all_brands",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d44c772a48298daa56e63f7fcbcc24811952e309",
        "_cell_guid": "407ed7c9-aff2-4047-8e75-2c22fac0c7a6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "full_df.head(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fde1716492ced542a18e6c8488d257dc2c8c6ed7",
        "_cell_guid": "8886d95a-3d4e-4d57-bd53-bf814b76f49a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('number of brands', len(full_df.brand_name.unique()))\nprint('number of item condition', len(full_df.item_condition_id.unique()))\nprint('number of cat1', len(full_df.subcat_0.unique()))\nprint('number of cat2', len(full_df.subcat_1.unique()))\nprint('number of cat3', len(full_df.subcat_2.unique()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "77ebeee9d25278b39065cdf1c81c5da11583d125",
        "_cell_guid": "01d6b834-3887-4fc6-99be-f7e240f2f650",
        "trusted": false
      },
      "cell_type": "code",
      "source": "full_df.brand_name.fillna(value=\"missing\", inplace=True)\nfull_df[\"target\"] = np.log1p(full_df.price)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6dc3e8bc93fc26c40645f405360427b282f1cb80",
        "_cell_guid": "688678f1-b681-4eef-8d25-f096497d94b5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "nrow_test = len(train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a0cfa5a34052b60138bc04a56198d23fb8ded7a4",
        "_cell_guid": "80ff4120-e4c7-4212-81fa-a76ca5680a8e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "nm_tfidf = TfidfVectorizer(ngram_range=(1, 3),lowercase=True,max_df=0.95,min_df=10,max_features=1000)\nX_name = nm_tfidf.fit_transform(full_df['name'].values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ae61a6601ac25730ee6be241c03b9201dfddee9",
        "_cell_guid": "5ccd10e2-6c62-4c4f-b895-6590a0ef4596",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "desc_tfidf = TfidfVectorizer(ngram_range=(1, 3),lowercase=True,max_df=0.95,min_df=10,max_features=1000)\nX_desc = desc_tfidf.fit_transform(full_df['item_description'].values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "123bc939f3088a2ccc9054e21d547b811cf60441",
        "_cell_guid": "cd37a998-4446-4b11-8eb3-1ee6fba3d1ac",
        "trusted": false
      },
      "cell_type": "code",
      "source": "wb = CountVectorizer()\nX_category1 = wb.fit_transform(full_df['subcat_0'])\nX_category2 = wb.fit_transform(full_df['subcat_1'])\nX_category3 = wb.fit_transform(full_df['subcat_2'])\nX_others = full_df[['shipping','item_condition_id','desc_len','name_len']].values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b2e09974fc78b770be03f98391e9fbd65f0daf52",
        "_cell_guid": "77111d1b-3a98-426e-9457-dee74942414f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X_name.shape,X_desc.shape,X_category1.shape,X_category2.shape,X_category3.shape,X_others.shape\nX_others[0],X_category1[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "be73c00c1dd5f2c708c7188aab6148888cdd2913",
        "_cell_guid": "9072fed2-1518-4f64-b700-79c65c3eebaf",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from scipy.sparse import csr_matrix, hstack, coo_matrix\n\n# merge = np.hstack((X_name,X_desc,X_category1,X_category2,X_category3,X_others))\nmerge = hstack((X_name,X_desc,X_category1,X_category2,X_category3,csr_matrix(X_others))).tocsr()\nX_train = merge[:nrow_test]\nX_test = merge[nrow_test:]\n# y_train = full_df['target'].values\ny_train = full_df.iloc[:nrow_test,-1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "909761df1274a82fbd3511b3031c4c5797656b27",
        "_cell_guid": "5ca3a47e-c910-4b4d-b4be-8fda648dc2b5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# ml_mapper = DataFrameMapper([\n#     ('brand_name',LabelEncoder()),\n#     ('subcat_0',LabelEncoder()),\n#     ('subcat_1',LabelEncoder()),\n#     ('subcat_2',LabelEncoder()),\n#     ('desc_len',None),\n#     ('name_len',None),\n#     ('shipping',None),\n#     ('item_description',TfidfVectorizer(ngram_range=(1, 3),lowercase=True,max_df=0.95,min_df=10,max_features=1000)),\n#     ('name',TfidfVectorizer(ngram_range=(1, 3),lowercase=True,max_df=0.95,min_df=10,max_features=1000)),\n#     ('train_id',None),\n#     ('test_id',None),\n#     ('target',None)\n# ])\n\n# processed_df = ml_mapper.fit_transform(full_df)\n# mapper.transformed_names_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b32a327bfd8ba691c0b357234b31f9210b4a626",
        "_cell_guid": "11005bce-f14a-479f-83fd-fc574b2ef4c7",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Try Xgboost , Lightgbm ,wordbatch\nimport lightgbm as lgb\n# http://lightgbm.readthedocs.io/en/latest/Python-API.html\n\nd_train = lgb.Dataset(X_train, label=y_train)\nparams = {}\nparams['learning_rate'] = 0.03\nparams['data_random_seed'] = 1\nparams['objective'] = 'regression'\nparams['metric'] = 'RMSE'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 10\nparams['min_data'] = 50\nparams['max_depth'] = 10\n\n# clf = lgb.train(params, d_train, 100)\n\nclf = lgb.cv(params, d_train,num_boost_round=1000,early_stopping_rounds=20,verbose_eval=20,nfold=4 )\n\ny_pred=clf.predict(x_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "92e44e154800773d173f1ce313e5bd94427f30ab",
        "_cell_guid": "e6de27ef-1201-40f0-bea4-4cf948014055",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# print(\"Fitting Ridge model on training examples...\")\n# ridge_model = Ridge(\n#     solver='auto', fit_intercept=True, alpha=1.0,\n#     max_iter=100, normalize=False, tol=0.05, random_state = 1,\n# )\n# ridge_modelCV = RidgeCV(\n#     fit_intercept=True, alphas=[5.0],\n#     normalize=False, cv = 2, scoring='neg_mean_squared_error',\n# )\n# ridge_model.fit(X_train, Y_train)\n# ridge_modelCV.fit(X_train, Y_train)\n\n# Y_dev_preds_ridge = ridge_model.predict(X_dev)\n# Y_dev_preds_ridge = Y_dev_preds_ridge.reshape(-1, 1)\n# print(\"RMSL error on dev set:\", rmsle(Y_dev, Y_dev_preds_ridge))\n\n# Y_dev_preds_ridgeCV = ridge_modelCV.predict(X_dev)\n# Y_dev_preds_ridgeCV = Y_dev_preds_ridgeCV.reshape(-1, 1)\n# print(\"CV RMSL error on dev set:\", rmsle(Y_dev, Y_dev_preds_ridgeCV))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e33959a286e0be034312638892e0729ae0218725",
        "_cell_guid": "27d213d5-3eee-474a-a8ca-afb2dfb01a0c"
      },
      "cell_type": "markdown",
      "source": "**RNN******\n"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b7b03d36012aa1e60544c6e8c0a5bc4b4c4a765f",
        "_cell_guid": "16585fde-f2a6-44e2-b90a-cf96ebc69e80",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Transforming text data to sequences...\")\nraw_text = np.hstack([full_df.item_description.str.lower(), full_df.name.str.lower(), full_df.category_name.str.lower()])\n\nprint(\"   Fitting tokenizer...\")\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\n\nprint(\"   Transforming text to sequences...\")\nfull_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\nfull_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n# full_df['seq_category'] = tok_raw.texts_to_sequences(full_df.category_name.str.lower())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b9379b66ef88ad2ad9dc5b38440dddbb6673bed9",
        "_cell_guid": "b0184ec7-560a-411f-9e93-98e183bc9c1c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "rnn_mapper = DataFrameMapper([\n    ('seq_name',None),\n    ('seq_item_description',None),\n    ('brand_name',LabelEncoder()),\n    ('item_condition_id',None),\n    ('shipping',None),\n    ('subcat_0',LabelEncoder()),\n    ('subcat_1',LabelEncoder()),\n    ('subcat_2',LabelEncoder()),\n    ('train_id',None),\n    ('test_id',None),\n    ('target',None)\n],df_out=True)\n\nrnn_df = rnn_mapper.fit_transform(full_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "2d3260477b10494baa6739460591f084da1ff705",
        "_cell_guid": "e0050e3d-9603-4d30-b3e4-4e7dc43084d4",
        "trusted": false
      },
      "cell_type": "code",
      "source": "trainset = rnn_df[pd.isnull(rnn_df.test_id)]\ndel trainset['test_id']\ndel trainset['train_id']\ntestset = rnn_df[pd.isnull(rnn_df.train_id)]\ndel testset['train_id']\ndel testset['test_id']\ndel testset['target']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "c488c095903b25a0e5a9687642e1b8a1cf404860",
        "_cell_guid": "abdee03f-19f7-444b-a94c-596216907a93",
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(trainset),len(testset)\nfull_df.seq_name.max(),np.max(full_df.seq_name.max())\nfull_df.seq_item_description.apply(lambda x:len(x)).max()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "f07e07336ba62ed440703487f7a36bd79e1a4709",
        "_cell_guid": "c5f49b81-db4a-4351-845d-0843998fc6ab",
        "trusted": false
      },
      "cell_type": "code",
      "source": "MAX_NAME_SEQ = 17 #17\nMAX_ITEM_DESC_SEQ = 100 #269\nMAX_TEXT = np.max([\n    np.max(rnn_df.seq_name.max()),\n    np.max(rnn_df.seq_item_description.max()),\n#     np.max(full_df.seq_category.max()),\n]) + 100\nMAX_BRAND = np.max(rnn_df.brand_name.max()) + 1\nMAX_CONDITION = np.max(rnn_df.item_condition_id.max()) + 1\n# MAX_DESC_LEN = np.max(rnn_df.desc_len.max()) + 1\n# MAX_NAME_LEN = np.max(rnn_df.name_len.max()) + 1\nMAX_SUBCAT_0 = np.max(rnn_df.subcat_0.max()) + 1\nMAX_SUBCAT_1 = np.max(rnn_df.subcat_1.max()) + 1\nMAX_SUBCAT_2 = np.max(rnn_df.subcat_2.max()) + 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "99fe1c9ee6a4edb406e9f929f8842044b8a1d8b9",
        "_cell_guid": "890d9336-f77b-40f9-a6cc-c53d12fe71e2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_rnn_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ),\n        'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n        'brand_name': np.array(dataset.brand_name),\n#         'category': np.array(dataset.category),\n#         'category_name': pad_sequences(dataset.seq_category, maxlen=MAX_CATEGORY_SEQ),\n        'item_condition': np.array(dataset.item_condition_id),\n        'num_vars': np.array(dataset.shipping),\n#         'desc_len': np.array(dataset[[\"desc_len\"]]),\n#         'name_len': np.array(dataset[[\"name_len\"]]),\n        'subcat_0': np.array(dataset.subcat_0),\n        'subcat_1': np.array(dataset.subcat_1),\n        'subcat_2': np.array(dataset.subcat_2),\n    }\n    return X\n\n\ntrain,valid = train_test_split(trainset, random_state=123, train_size=0.8)\nX_test = testset.values\n\nX_train = get_rnn_data(train.iloc[:,0:-1].copy())\ny_train = train.iloc[:,-1].values\n# Y_train = train.target.values.reshape(-1, 1)\n\nX_valid = get_rnn_data(valid.iloc[:,0:-1].copy())\ny_valid = valid.iloc[:,-1].values\n# Y_dev = dev.target.values.reshape(-1, 1)\n\nX_test = get_rnn_data(testset)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b404605bb3eeb623c90472caea15824eefb680c9",
        "_cell_guid": "e162c8b7-bde3-47d5-b62b-8fb3915b3afc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# MAX_SUBCAT_0,MAX_SUBCAT_1,MAX_SUBCAT_2\n# rnn_df.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "a9fbe39271fbafb2490e9064a5158b20e94d012f",
        "_cell_guid": "91430828-afb8-42e3-8018-8d1fbce8cd2a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# RNN\n\n# set seed again in case testing models adjustments by looping next 2 blocks\nnp.random.seed(123)\n\ndef new_rnn_model(lr=0.001, decay=0.0):\n    # Inputs\n    name = Input(shape=(17,), name=\"name\")\n    item_desc = Input(shape=(100,), name=\"item_desc\")\n    brand_name = Input(shape=(1,), name=\"brand_name\")\n    item_condition = Input(shape=(1,), name=\"item_condition\")\n    num_vars = Input(shape=(1,), name=\"num_vars\")\n#     desc_len = Input(shape=[1], name=\"desc_len\")\n#     name_len = Input(shape=[1], name=\"name_len\")\n    subcat_0 = Input(shape=(1,), name=\"subcat_0\")\n    subcat_1 = Input(shape=(1,), name=\"subcat_1\")\n    subcat_2 = Input(shape=(1,), name=\"subcat_2\")\n\n    # Embeddings layers (adjust outputs to help model)\n    emb_name = Embedding(MAX_TEXT, 20)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n#     emb_category_name = Embedding(MAX_TEXT, 20)(category_name)\n#     emb_category = Embedding(MAX_CATEGORY, 10)(category)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n#     emb_desc_len = Embedding(MAX_DESC_LEN, 5)(desc_len)\n#     emb_name_len = Embedding(MAX_NAME_LEN, 5)(name_len)\n    emb_subcat_0 = Embedding(MAX_SUBCAT_0, 10)(subcat_0)\n    emb_subcat_1 = Embedding(MAX_SUBCAT_1, 10)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_SUBCAT_2, 10)(subcat_2)\n    \n\n    # rnn layers (GRUs are faster than LSTMs and speed is important here)\n    rnn_layer1 = GRU(16) (emb_item_desc)\n    rnn_layer2 = GRU(8) (emb_name)\n#     rnn_layer3 = GRU(8) (emb_category_name)\n\n    # main layers\n    main_l = concatenate([\n        Flatten() (emb_brand_name)\n#         , Flatten() (emb_category)\n        , Flatten() (emb_item_condition)\n#         , Flatten() (emb_desc_len)\n#         , Flatten() (emb_name_len)\n        , Flatten() (emb_subcat_0)\n        , Flatten() (emb_subcat_1)\n        , Flatten() (emb_subcat_2)\n        , rnn_layer1\n        , rnn_layer2\n#         , rnn_layer3\n        , num_vars\n    ])\n    # (incressing the nodes or adding layers does not effect the time quite as much as the rnn layers)\n    main_l = Dropout(0.1)(Dense(512,kernel_initializer='normal',activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(256,kernel_initializer='normal',activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(128,kernel_initializer='normal',activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(64,kernel_initializer='normal',activation='relu') (main_l))\n\n    # the output layer.\n    output = Dense(1, activation=\"linear\") (main_l)\n    \n    model = Model([name, item_desc, brand_name , item_condition, subcat_0, subcat_1, subcat_2], output)\n\n    optimizer = Adam(lr=lr, decay=decay)\n    # (mean squared error loss function works as well as custom functions)  \n    model.compile(loss = 'mse', optimizer = optimizer)\n\n    return model\n\nmodel = new_rnn_model()\nmodel.summary()\n# del model\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "324178f83d1cbe972c28c2998fd800f6943d61ee",
        "_cell_guid": "e1fb769e-9454-4106-b832-37eca308d640",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Set hyper parameters for the model.\nBATCH_SIZE = 512 * 3\nepochs = 2\n\n# Calculate learning rate decay.\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = int(len(train) / BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.005, 0.001\nlr_decay = exp_decay(lr_init, lr_fin, steps)\n\n# Create model and fit it with training dataset.\n# rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\nrnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\nrnn_model.fit(\n        X_train, y_train, epochs=epochs, batch_size=BATCH_SIZE,\n        validation_data=(X_valid, y_valid), verbose=1,\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "66ab15d60aa58f67c6c2e5cf5fb0acf795ed2687",
        "_cell_guid": "cb8bc9ea-1570-451e-8179-df5268e7414b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "preds = rnn_model.predict(X_test)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "a67b1ba2b470792db820154fdc6fbd392614d57a",
        "_cell_guid": "0ef79b7a-897a-47a9-aba9-1e6c1b662dee",
        "trusted": false
      },
      "cell_type": "code",
      "source": "preds_df = pd.DataFrame(preds)\npreds_df.reset_index(inplace=True)\npreds_df.columns = ['test_id','price']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b70a76e364c7425eae3ebb96122f88d8b690b44c",
        "_cell_guid": "6c54af1d-69db-48e8-b75e-dc44dd9976e8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "preds_df['price'] = preds_df.price.apply(lambda x:np.expm1(x))\npreds_df.to_csv('rnn_submit_1.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "465d07aac1ff893cd54b1b4ab47ef71db25687d3",
        "_cell_guid": "1ded46e0-ee38-450a-b854-79f98ade8c58",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# https://github.com/anttttti/Wordbatch",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}