{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# Hierarchal RNN\n",
    "\n",
    "This kernel is a somewhat improved version of [Keras - Bidirectional LSTM baseline](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051) along with some additional documentation of the steps. (NB: this notebook has been re-run on the new test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fast Text\n",
    "# Increase the glove Embedding\n",
    "# Use Fast Text to generate the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py\n",
    "# https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf\n",
    "# https://github.com/EdGENetworks/attention-networks-for-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import Conv1D, MaxPooling1D,Merge, GRU, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "import fastText\n",
    "\n",
    "from nltk import tokenize\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec\n",
    "from keras.initializers import zero\n",
    "from keras.initializers import RandomNormal\n",
    "import tensorflow as tf\n",
    "\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft = fastText.load_model('wv/wiki.en.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 512\n",
    "# MAX_SENTS = 20\n",
    "EMBEDDING_DIM = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    \"\"\"\n",
    "    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Replace ips\n",
    "    s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n",
    "    # Isolate punctuation\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
    "    # Remove some special characters\n",
    "    \n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n「」¤]\\xa0)', ' ', s)\n",
    "    # Replace numbers and symbols with language\n",
    "    s = s.replace('&', ' and ')\n",
    "    s = s.replace('@', ' at ')\n",
    "    s = s.replace('0', ' zero ')\n",
    "    s = s.replace('1', ' one ')\n",
    "    s = s.replace('2', ' two ')\n",
    "    s = s.replace('3', ' three ')\n",
    "    s = s.replace('4', ' four ')\n",
    "    s = s.replace('5', ' five ')\n",
    "    s = s.replace('6', ' six ')\n",
    "    s = s.replace('7', ' seven ')\n",
    "    s = s.replace('8', ' eight ')\n",
    "    s = s.replace('9', ' nine ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "train[\"comment_text\"].fillna(\"_empty_\",inplace=True)\n",
    "list_sentences_train = train[\"comment_text\"].apply(lambda x:normalize(x)).values\n",
    "test[\"comment_text\"].fillna(\"_empty_\",inplace=True)\n",
    "list_sentences_test = test[\"comment_text\"].apply(lambda x:normalize(x)).values\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='_oov_')\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only for Option 3\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=MAX_SENT_LENGTH)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 20, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only for Option 1 and 2\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for i in list_sentences_train:\n",
    "    sentences = tokenize.sent_tokenize(i)\n",
    "    reviews.append(sentences)\n",
    "    \n",
    "# Zero paddings \n",
    "data = np.zeros((len(list_sentences_train), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            # same as split + lower + punctuation removal\n",
    "#             wordTokens = text_to_word_sequence(sent)\n",
    "            wordTokens = sent.lower().split(' ')\n",
    "#             k=0\n",
    "            for k , word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH :\n",
    "                    try :\n",
    "                        data_i = tokenizer.word_index[word]\n",
    "                    except KeyError:\n",
    "#                         print(word)\n",
    "                        data_i = 0\n",
    "                    data[i,j,k] = data_i\n",
    "#                     k=k+1                    \n",
    "\n",
    "del list_sentences_train\n",
    "gc.collect()\n",
    "\n",
    "t_reviews = []\n",
    "\n",
    "# Test Set \n",
    "for i in list_sentences_test:\n",
    "    sentences = tokenize.sent_tokenize(i)\n",
    "    t_reviews.append(sentences)\n",
    "    \n",
    "# Zero paddings \n",
    "t_data = np.zeros((len(list_sentences_test), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(t_reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            # same as split + lower + punctuation removal\n",
    "#             wordTokens = text_to_word_sequence(sent)\n",
    "            wordTokens = sent.lower().split(' ')\n",
    "#             k=0\n",
    "            for k , word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH :\n",
    "                    try :\n",
    "                        data_i = tokenizer.word_index[word]\n",
    "                    except KeyError:\n",
    "#                         print(word)\n",
    "                        data_i = 0\n",
    "                    t_data[i,j,k] = data_i\n",
    "#                     k=k+1                    \n",
    "\n",
    "del list_sentences_test\n",
    "gc.collect()\n",
    "\n",
    "# Validation Set\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = y[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "# print y_train.sum(axis=0)\n",
    "# print y_val.sum(axis=0)\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# cachedStop =  stopwords.words('english')\n",
    "# pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "# def cleanwords(sent):\n",
    "#     return ' '.join([word.lower() for word in sent.lower().split() if word not in cachedStop ])\n",
    "    # return pattern.sub('', sent.lower())\n",
    "\n",
    "# def cleanchars(sent):\n",
    "#     return sent.translate(translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))+1\n",
    "embedding_matrix = np.random.normal(-0.0039050116, 0.38177028, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = ft.get_word_vector(word).astype('float32') # out of word vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 15, 500)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 200)           38832800  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 1206      \n",
      "=================================================================\n",
      "Total params: 39,074,806\n",
      "Trainable params: 39,074,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(MAX_NB_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "preds = Dense(6, activation='sigmoid')(l_lstm_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(2, 4) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment\n",
    "# x = K.placeholder(shape=(2, 3))\n",
    "# y = K.placeholder(shape=(3, 4))\n",
    "# xy = tf.keras.backend.dot(x, y)\n",
    "# xy\n",
    "\n",
    "# import numpy as np\n",
    "# x = np.zeros([500,200])\n",
    "# x.shape[-1]\n",
    "\n",
    "# init = initializers.get('normal')\n",
    "# w = init((200,))\n",
    "# K.expand_dims(w).shape\n",
    "\n",
    "# init\n",
    "\n",
    "#batch, time(max_len),word_dim\n",
    "# x = tf.placeholder(np.float32,(16,500,200))\n",
    "# W1 = tf.placeholder(np.float32,(200,500))\n",
    "# y = tf.keras.backend.dot(x,W1)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 100)           58008500  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20, 100)           45300     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 100)           10100     \n",
      "_________________________________________________________________\n",
      "att_layer_2 (AttLayer)       (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 58,069,606\n",
      "Trainable params: 58,069,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building Hierachical Attention network\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        self.attention_size = 50\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = tf.Variable(tf.random_normal([input_shape[-1], self.attention_size], stddev=0.1))\n",
    "        self.B = tf.Variable(tf.random_normal([self.attention_size], stddev=0.1))\n",
    "        self.U = tf.Variable(tf.random_normal([self.attention_size], stddev=0.1))\n",
    "        self.trainable_weights = [self.W,self.B,self.U]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.B)\n",
    "        vu = tf.tensordot(v, self.U, axes=1)  # (B,T) shape\n",
    "        alphas = tf.nn.softmax(vu)         # (B,T) shape\n",
    "        output = tf.reduce_sum(x * tf.expand_dims(alphas, -1), 1)\n",
    "        \n",
    "        return output\n",
    "#         eij = tf.squeeze(tf.keras.backend.dot(x, tf.keras.backend.expand_dims(self.W,-1)), axis=-1)\n",
    "        \n",
    "#         ai = tf.exp(eij)\n",
    "#         weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "#         weights = tf.keras.backend.expand_dims(ai/tf.keras.backend.sum(ai, axis=1),-1)\n",
    "        # replace dimshuffle with tf.expand_dims()\n",
    "        \n",
    "#         weighted_input = x*weights\n",
    "#         return tf.keras.backend.sum(weighted_input,axis=1)\n",
    "#         return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(50, return_sequences=True))(embedded_sequences)\n",
    "# returns 500 vectors and applies a dense layer of 200 to each\n",
    "l_dense = TimeDistributed(Dense(100))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "# output (batch,500,100) --> Highlight the essence of the word\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "# returns 15 vectors and applies a dense layer of 200 to each vector : (15,500) * (500,100)\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(50, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(100))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(6, activation='sigmoid')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/2\n",
      "143614/143614 [==============================] - 3613s 25ms/step - loss: 0.0683 - acc: 0.9779 - val_loss: 0.0456 - val_acc: 0.9829\n",
      "Epoch 2/2\n",
      "143614/143614 [==============================] - 3603s 25ms/step - loss: 0.0408 - acc: 0.9844 - val_loss: 0.0477 - val_acc: 0.9817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3fd4fd3d68>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x_val, y_val,epochs=1, batch_size=16)\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('lstm_attention_1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
    "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
   },
   "source": [
    "And finally, get predictions for the test set and prepare a submission CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28ce30e3-0f21-48e5-af3c-7e5512c9fbdc",
    "_uuid": "e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4896/153164 [..............................] - ETA: 12:48"
     ]
    }
   ],
   "source": [
    "y_test = model.predict(t_data, batch_size=32, verbose=1)\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('lstm_attention_baseline_g300.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        self.attention_size = 50\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = tf.Variable(tf.random_normal([input_shape[-1], self.attention_size], stddev=0.1))\n",
    "        self.B = tf.Variable(tf.random_normal([self.attention_size], stddev=0.1))\n",
    "        self.U = tf.Variable(tf.random_normal([self.attention_size], stddev=0.1))\n",
    "        self.trainable_weights = [self.W,self.B,self.U]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.B)\n",
    "        vu = tf.tensordot(v, self.U, axes=1)  # (B,T) shape\n",
    "        alphas = tf.nn.softmax(vu)         # (B,T) shape\n",
    "        output = tf.reduce_sum(x * tf.expand_dims(alphas, -1), 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(50, return_sequences=True))(embedded_sequences)\n",
    "# returns 500 vectors and applies a dense layer of 200 to each\n",
    "l_dense = TimeDistributed(Dense(100))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "l_att = Dropout(0.1)(l_att)\n",
    "preds = Dense(6, activation='sigmoid')(l_att)\n",
    "model = Model(sentence_input, preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2,validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = model.predict([X_te], batch_size=32, verbose=1)\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('lstm_attention_baseline_ft300.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample_submission.to_csv('base_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_submission = pd.read_csv('data/sample_submission.csv')\n",
    "# len(test_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline Score\n",
    "# loss: 0.0417 - acc: 0.9840 - val_loss: 0.0451 - val_acc: 0.9829 --> AUC : 0.9787\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
